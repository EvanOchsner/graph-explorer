{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Explorer - Parquet Visualization Example\n",
    "\n",
    "This notebook demonstrates how to use the Graph Explorer Python helper\n",
    "to visualize relationship data from Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Import graph explorer helpers\n",
    "from graph_explorer import process_parquet_for_graph, visualize_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Sample Parquet Data\n",
    "\n",
    "For this example, we'll create a synthetic dataset of relationships between entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset\n",
    "# In real-world usage, you'd load your own Parquet file instead\n",
    "\n",
    "# Sample entities\n",
    "entities = [\n",
    "    \"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Evan\", \"Fiona\", \"George\", \n",
    "    \"Hannah\", \"Ian\", \"Julia\", \"Kevin\", \"Laura\", \"Michael\", \"Natalie\"\n",
    "]\n",
    "\n",
    "# Relationship types\n",
    "rel_types = [\"friend\", \"colleague\", \"family\", \"classmate\", \"neighbor\", \"partner\"]\n",
    "\n",
    "# Create random relationships\n",
    "np.random.seed(42)  # For reproducibility\n",
    "rows = []\n",
    "\n",
    "for _ in range(100):  # Generate 100 random relationships\n",
    "    # Get two random entities (ensure they're different)\n",
    "    entity1, entity2 = np.random.choice(entities, size=2, replace=False)\n",
    "    \n",
    "    # Get a random relationship type\n",
    "    rel = np.random.choice(rel_types)\n",
    "    \n",
    "    # Generate a random strength score\n",
    "    strength = round(np.random.random(), 2)\n",
    "    \n",
    "    # Generate a random frequency\n",
    "    freq = np.random.randint(1, 50)\n",
    "    \n",
    "    # Add to our dataset\n",
    "    rows.append({\n",
    "        \"entity1\": entity1,\n",
    "        \"entity2\": entity2,\n",
    "        \"relationship\": rel,\n",
    "        \"strength\": strength,\n",
    "        \"frequency\": freq,\n",
    "        \"active\": np.random.choice([True, False], p=[0.8, 0.2])\n",
    "    })\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Parquet file\n",
    "parquet_path = \"sample_relationships.parquet\"\n",
    "df.to_parquet(parquet_path)\n",
    "\n",
    "print(f\"Saved data to {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Analyze the Parquet Data\n",
    "\n",
    "In a real-world scenario, you'd load your existing Parquet file and perform some analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Parquet file\n",
    "df = pd.read_parquet(parquet_path)\n",
    "\n",
    "# Basic analysis\n",
    "print(f\"Dataset has {len(df)} rows and {len(df.columns)} columns\")\n",
    "print(f\"Columns: {', '.join(df.columns)}\")\n",
    "print(f\"\\nRelationship types:\")\n",
    "print(df[\"relationship\"].value_counts())\n",
    "print(f\"\\nEntity counts:\")\n",
    "entities_count = pd.concat([df[\"entity1\"], df[\"entity2\"]]).value_counts().head(10)\n",
    "print(entities_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter a Subset for Visualization\n",
    "\n",
    "Let's extract a subset of the data that we want to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Extract and visualize all active relationships\n",
    "active_df = process_parquet_for_graph(\n",
    "    df,\n",
    "    source_col=\"entity1\",\n",
    "    target_col=\"entity2\",\n",
    "    edge_type_col=\"relationship\",\n",
    "    filters={\"active\": True}\n",
    ")\n",
    "\n",
    "print(f\"Processed {len(active_df)} active relationships for visualization\")\n",
    "active_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Extract only strong relationships (strength > 0.7)\n",
    "strong_df = process_parquet_for_graph(\n",
    "    df,\n",
    "    source_col=\"entity1\",\n",
    "    target_col=\"entity2\",\n",
    "    edge_type_col=\"relationship\",\n",
    "    filters={\n",
    "        \"active\": True,\n",
    "        \"strength\": {\"operator\": \">\", \"value\": 0.7}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Processed {len(strong_df)} strong relationships for visualization\")\n",
    "strong_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Extract relationships involving a specific entity\n",
    "alice_df = df[\n",
    "    (df[\"entity1\"] == \"Alice\") | (df[\"entity2\"] == \"Alice\")\n",
    "]\n",
    "\n",
    "alice_graph_df = process_parquet_for_graph(\n",
    "    alice_df,\n",
    "    source_col=\"entity1\",\n",
    "    target_col=\"entity2\",\n",
    "    edge_type_col=\"relationship\"\n",
    ")\n",
    "\n",
    "print(f\"Found {len(alice_graph_df)} relationships involving Alice\")\n",
    "alice_graph_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize the Data\n",
    "\n",
    "Now let's visualize these different subsets in the Graph Explorer application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the Graph Explorer app is running at http://localhost:3000\n",
    "# before executing this cell\n",
    "\n",
    "# Visualize all active relationships\n",
    "visualize_graph(active_df, method=\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize only strong relationships\n",
    "visualize_graph(strong_df, method=\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Alice's relationship network\n",
    "visualize_graph(alice_graph_df, method=\"url\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Example: Two-hop Neighborhood\n",
    "\n",
    "Let's extract a more complex subgraph: all entities within two relationship hops of a given entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_hop_network(df, entity, source_col=\"entity1\", target_col=\"entity2\"):\n",
    "    \"\"\"Extract all nodes within 2 hops of the given entity\"\"\"\n",
    "    # First hop: direct connections\n",
    "    direct = df[(df[source_col] == entity) | (df[target_col] == entity)]\n",
    "    \n",
    "    # Get all directly connected entities\n",
    "    connected = set()\n",
    "    connected.update(direct[direct[source_col] == entity][target_col].tolist())\n",
    "    connected.update(direct[direct[target_col] == entity][source_col].tolist())\n",
    "    \n",
    "    # Second hop: connections of connected entities\n",
    "    second_hop = df[\n",
    "        (df[source_col].isin(connected) & ~(df[target_col] == entity)) | \n",
    "        (df[target_col].isin(connected) & ~(df[source_col] == entity))\n",
    "    ]\n",
    "    \n",
    "    # Combine direct and second-hop connections\n",
    "    result = pd.concat([direct, second_hop]).drop_duplicates()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Get Kevin's two-hop network\n",
    "kevin_network = get_two_hop_network(df, \"Kevin\")\n",
    "print(f\"Kevin's network has {len(kevin_network)} relationships\")\n",
    "\n",
    "# Process for visualization\n",
    "kevin_graph = process_parquet_for_graph(\n",
    "    kevin_network,\n",
    "    source_col=\"entity1\",\n",
    "    target_col=\"entity2\",\n",
    "    edge_type_col=\"relationship\"\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "visualize_graph(kevin_graph, method=\"url\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-world Application: Working with Large Parquet Files\n",
    "\n",
    "In a real-world scenario with very large Parquet files, you'd typically:\n",
    "\n",
    "1. Load the Parquet file with filters to reduce memory usage\n",
    "2. Perform exploratory analysis to identify interesting subgraphs\n",
    "3. Extract the relevant subset for visualization\n",
    "\n",
    "The code below shows a sketch of this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode for working with large parquet files\n",
    "# (This is not meant to be executed)\n",
    "\n",
    "'''\n",
    "# 1. Load only the necessary columns and with filters\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Define columns to read\n",
    "columns = [\"source_id\", \"target_id\", \"relationship_type\", \"weight\"]\n",
    "\n",
    "# Read with predicate pushdown (filtering at file read time)\n",
    "# This is much more efficient than loading the entire file\n",
    "parquet_file = pq.ParquetFile(\"very_large_dataset.parquet\")\n",
    "df = parquet_file.read(columns=columns, filters=[(\"date\", \"=\", \"2023-01-01\")]).to_pandas()\n",
    "\n",
    "# 2. Find an interesting subset through analysis\n",
    "# For example, finding high centrality nodes:\n",
    "from collections import Counter\n",
    "node_counts = Counter(df[\"source_id\"].tolist() + df[\"target_id\"].tolist())\n",
    "top_nodes = [node for node, count in node_counts.most_items()[:10]]\n",
    "\n",
    "# 3. Extract the subgraph for one of these top nodes (with some maximum size limit)\n",
    "central_node = top_nodes[0]\n",
    "k_hop_neighborhood = get_k_hop_neighborhood(df, central_node, k=2, max_edges=1000)\n",
    "\n",
    "# 4. Process and visualize\n",
    "graph_df = process_parquet_for_graph(\n",
    "    k_hop_neighborhood,\n",
    "    source_col=\"source_id\",\n",
    "    target_col=\"target_id\",\n",
    "    edge_type_col=\"relationship_type\",\n",
    "    filters={\"weight\": {\"operator\": \">\", \"value\": 0.5}},\n",
    "    max_records=500\n",
    ")\n",
    "\n",
    "visualize_graph(graph_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "\n",
    "1. Work with Parquet data in Python\n",
    "2. Extract meaningful relationship subsets\n",
    "3. Process the data for network visualization\n",
    "4. Send the data directly to the Graph Explorer web application\n",
    "\n",
    "With these tools, you can easily explore network relationships in large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}